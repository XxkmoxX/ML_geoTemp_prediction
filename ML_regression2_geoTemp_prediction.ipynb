{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76190361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['well_sample', 'temp_measured', 'pH', 'Na ', 'K', 'Ca', 'Mg', 'Cl',\n",
      "       'SO4'],\n",
      "      dtype='object')\n",
      "Features of dataset: Index(['pH', 'Na ', 'K', 'Ca', 'Mg', 'Cl', 'SO4'], dtype='object')\n",
      "Number of compenents in features: 7\n",
      "0    137\n",
      "1    137\n",
      "2    137\n",
      "3    137\n",
      "4    150\n",
      "5    116\n",
      "6    165\n",
      "7    140\n",
      "8    115\n",
      "9    115\n",
      "Name: temp_measured, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from tabulate import tabulate\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_squared_log_error, mean_absolute_error, make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('csv/final_dataset.csv')\n",
    "print(data.columns)\n",
    "\n",
    "# Separate features and response variables\n",
    "X = data.iloc[:, 2:]                                # features\n",
    "Y = data['temp_measured']                           # response variable: geothermal reservoir measured temperature\n",
    "print(f'Features of dataset: {X.columns}')\n",
    "print(f'Number of compenents in features: {X.shape[1]}')\n",
    "print(Y.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a95f971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+\n",
      "| Eval_metrics  | MLP Regressor |\n",
      "+---------------+---------------+\n",
      "|   R2 Score    |    0.5649     |\n",
      "|      MSE      |   3282.9729   |\n",
      "|      MAE      |    0.1494     |\n",
      "|     MSLE      |    37.1181    |\n",
      "|     MRSE      |    0.2125     |\n",
      "| Training time |     29.01     |\n",
      "+---------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "### Scikit-learn MLP Implementation ..... probar RandomizedSearchCV u Optuna\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "start_time_mlp = time.time()\n",
    "\n",
    "x_train_mlp, x_test_mlp, y_train_log_mlp, y_test_log_mlp = train_test_split(X, np.log(Y), test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "x_train_mlp = scaler.fit_transform(x_train_mlp)\n",
    "x_test_mlp = scaler.transform(x_test_mlp)\n",
    "\n",
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes=(1024, 512, 256),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.001,  # Regularización L2\n",
    "    learning_rate='adaptive',\n",
    "    learning_rate_init=0.0001,\n",
    "    max_iter=1000,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.2,\n",
    "    n_iter_no_change=20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "mlp.fit(x_train_mlp, y_train_log_mlp)\n",
    "\n",
    "end_time_mlp = time.time()\n",
    "\n",
    "y_pred_log_mlp = mlp.predict(x_test_mlp)\n",
    "y_pred_test_mlp = np.exp(y_pred_log_mlp)\n",
    "y_test_mlp_orig = np.exp(y_test_log_mlp)\n",
    "\n",
    "def mean_relative_squared_error(Y_true, Y_pred):\n",
    "    return np.mean(((Y_true - Y_pred) / Y_true) ** 2)\n",
    "\n",
    "\n",
    "r2_mlp = r2_score(y_test_mlp_orig, y_pred_test_mlp)\n",
    "mse_mlp = mean_squared_error(y_test_mlp_orig, y_pred_test_mlp)\n",
    "mslr_mlp = mean_squared_log_error(y_test_mlp_orig, y_pred_test_mlp)\n",
    "mae_mlp = mean_absolute_error(y_test_mlp_orig, y_pred_test_mlp)\n",
    "mrse_mlp = mean_relative_squared_error(y_test_mlp_orig, y_pred_test_mlp)\n",
    "\n",
    "training_time_mlp = end_time_mlp - start_time_mlp\n",
    "\n",
    "mlp_metrics = {\n",
    "    'Eval_metrics': ['R2 Score', 'MSE', 'MAE', 'MSLE', 'MRSE', 'Training time'],\n",
    "    'MLP Regressor': [r2_mlp, mse_mlp, mslr_mlp, mae_mlp, mrse_mlp, training_time_mlp]\n",
    "}\n",
    "\n",
    "df_mlp = pd.DataFrame(mlp_metrics)\n",
    "df_mlp.to_csv('metrics/metrics_mlp.csv', index=False)\n",
    "\n",
    "print(tabulate(df_mlp.round(4), headers='keys', tablefmt='pretty', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9853620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 11:27:21.826822: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-08-25 11:27:21.889732: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-08-25 11:27:21.908086: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-25 11:27:22.025914: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-25 11:27:23.140654: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/chris/venv/cdd/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "/home/chris/venv/cdd/lib/python3.12/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "+---------------+-------------------+\n",
      "| Eval_metrics  | Elastic-Net Model |\n",
      "+---------------+-------------------+\n",
      "|   R2 Score    |      0.5374       |\n",
      "|      MSE      |     3301.1821     |\n",
      "|      MAE      |      40.4601      |\n",
      "|     MSLR      |      0.1441       |\n",
      "|     MRSE      |      0.2365       |\n",
      "| Training time |      45.4058      |\n",
      "+---------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "### Neural Network implementation (Keras)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import RobustScaler  # Better for handling outliers\n",
    "\n",
    "\n",
    "start_time_k = time.time()\n",
    "\n",
    "x_train_k, x_test_k, y_train_log_k, y_test_log_k = train_test_split(X, np.log(Y), test_size=0.2, random_state=36)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "x_train_k = scaler.fit_transform(x_train_k)\n",
    "x_test_k = scaler.transform(x_test_k)\n",
    "\n",
    "# Define neural networks architecture with LeakyReLU activation\n",
    "model = Sequential([\n",
    "    # Input layer\n",
    "    Dense(512, input_dim=x_train_k.shape[1], kernel_regularizer=l2(0.01)),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Hidden layers\n",
    "    Dense(256, kernel_regularizer=l2(0.01)),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(128, kernel_regularizer=l2(0.01)),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    #Dense(64, kernel_regularizer=l2(0.01)),\n",
    "    #LeakyReLU(alpha=0.1),\n",
    "    #BatchNormalization(),\n",
    "    #Dropout(0.3),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "              loss='mean_squared_error', \n",
    "              metrics=['mean_absolute_error'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           patience=20, \n",
    "                           restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                              actor=0.2, \n",
    "                              patience=10, \n",
    "                              min_lr=1e-4)\n",
    "\n",
    "training = model.fit(x_train_k, y_train_log_k, epochs=2000, validation_split=0.2, batch_size=20,\n",
    "                     verbose=0, callbacks=[early_stop, reduce_lr])\n",
    "\n",
    "end_time_k = time.time()\n",
    "\n",
    "#model.save('keras_nn_model.h5')\n",
    "#print(\"Model saved to 'keras_nn_model.h5'.\")\n",
    "\n",
    "y_pred_test_log_k = model.predict(x_test_k)\n",
    "y_pred_train_log_k = model.predict(x_train_k)\n",
    "\n",
    "y_pred_test_k = np.exp(y_pred_test_log_k) \n",
    "y_pred_train_k = np.exp(y_pred_train_log_k)\n",
    "y_train_k = np.exp(y_train_log_k)\n",
    "y_test_k = np.exp(y_test_log_k)\n",
    "\n",
    "Y_test_k = np.squeeze(y_test_k)\n",
    "Y_pred_test_k = np.squeeze(y_pred_test_k)\n",
    "\n",
    "training_time_k = end_time_k - start_time_k\n",
    "\n",
    "def mean_relative_squared_error(y_true, y_pred_test):\n",
    "    return np.mean(((y_true - y_pred_test)/y_true)**2)\n",
    "\n",
    "r2_k = r2_score(y_test_k, y_pred_test_k)\n",
    "mse_k = mean_squared_error(y_test_k, y_pred_test_k)\n",
    "mae_k = mean_absolute_error(y_test_k, y_pred_test_k)\n",
    "mslr_k = mean_squared_log_error(y_test_k, y_pred_test_k)\n",
    "mrse_k = mean_relative_squared_error(Y_test_k, Y_pred_test_k)\n",
    "\n",
    "\n",
    "eval_metrics_k = {\n",
    "    'Eval_metrics': ['R2 Score', 'MSE', 'MAE', 'MSLR', 'MRSE', 'Training time'],\n",
    "    'Elastic-Net Model': [r2_k, mse_k, mae_k, mslr_k, mrse_k, training_time_k]\n",
    "}\n",
    "\n",
    "df_metrics_k = pd.DataFrame(eval_metrics_k)\n",
    "df_metrics_k.to_csv('metrics/metrics_nn.csv', index=False)\n",
    "\n",
    "print(tabulate(df_metrics_k.round(4), headers='keys', tablefmt='pretty', showindex=False))\n",
    "\n",
    "### reducir learning_rate de Adam de 0,01 a 0,001 mejor el r2, \n",
    "### de 0,001 a 0,0001 no lo mejoro y aumento mucho el tiempo.\n",
    "### Agregar input layer de 1024 no mejoro nada y aumento el tiempo\n",
    "### Con RobustScaler mejoro respecto de StandardScaler.\n",
    "### Prueba l2 de 0.01 a 0.001, no mejora nada, queda 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b81d26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Neural Network implementation (Keras) with Hyperparameter Tuning\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Import Keras and TensorFlow components\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_squared_log_error\n",
    "import keras_tuner as kt\n",
    "\n",
    "\n",
    "x_train_k, x_test_k, y_train_log_k, y_test_log_k = train_test_split(X, np.log(Y), test_size=0.2, random_state=36)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "x_train_k = scaler.fit_transform(x_train_k)\n",
    "x_test_k = scaler.transform(x_test_k)\n",
    "\n",
    "# 1. Define the model-building function for KerasTuner\n",
    "def build_model(hp):\n",
    "    \"\"\"Builds a Keras model with tunable hyperparameters.\"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Tune the number of units in the first Dense layer\n",
    "    hp_units_1 = hp.Int('units_1', min_value=256, max_value=1024, step=128)\n",
    "    model.add(Dense(units=hp_units_1, input_dim=x_train_k.shape[1], kernel_regularizer=l2(0.001)))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    # Tune the dropout rate\n",
    "    hp_dropout_1 = hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)\n",
    "    model.add(Dropout(rate=hp_dropout_1))\n",
    "\n",
    "    # Tune the number of hidden layers and their units\n",
    "    for i in range(hp.Int('num_layers', 1, 3)):\n",
    "        hp_units = hp.Int(f'units_{i+2}', min_value=64, max_value=512, step=64)\n",
    "        model.add(Dense(units=hp_units, kernel_regularizer=l2(0.001)))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(BatchNormalization())\n",
    "        hp_dropout = hp.Float(f'dropout_{i+2}', min_value=0.1, max_value=0.5, step=0.1)\n",
    "        model.add(Dropout(rate=hp_dropout))\n",
    "        \n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    # Tune the learning rate for the optimizer\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "                  loss='huber',\n",
    "                  metrics=['mean_absolute_error'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 2. Instantiate the tuner\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_mean_absolute_error',\n",
    "    max_epochs=100,  # Max epochs to train a model for\n",
    "    factor=3,\n",
    "    directory='keras_tuner_dir',\n",
    "    project_name='temperature_prediction'\n",
    ")\n",
    "\n",
    "# Define an early stopping callback to prevent overfitting during the search\n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=15)\n",
    "\n",
    "# 3. Run the hyperparameter search\n",
    "print(\"Starting hyperparameter search...\")\n",
    "tuner.search(x_train_k, y_train_log_k, epochs=100, validation_split=0.2, callbacks=[stop_early], verbose=1)\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. \n",
    "Optimal number of units in the first layer: {best_hps.get('units_1')}\n",
    "Optimal learning rate: {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n",
    "\n",
    "# 4. Build and train the final model with the best hyperparameters\n",
    "start_time_k = time.time()\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Define callbacks for the final training run\n",
    "final_early_stop = EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True, verbose=1)\n",
    "final_reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=20, min_lr=1e-6, verbose=1)\n",
    "final_checkpoint = ModelCheckpoint('best_nn_model_tuned.keras', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    x_train_k, \n",
    "    y_train_log_k, \n",
    "    epochs=1000,  # Train for a sufficient number of epochs\n",
    "    validation_split=0.2,\n",
    "    batch_size=32,\n",
    "    callbacks=[final_early_stop, final_reduce_lr, final_checkpoint],\n",
    "    verbose=2\n",
    ")\n",
    "end_time_k = time.time()\n",
    "\n",
    "# Load the best model weights saved by the checkpoint\n",
    "model.load_weights('best_nn_model_tuned.keras')\n",
    "print(\"Best tuned model loaded.\")\n",
    "\n",
    "# 5. Evaluate the final model\n",
    "y_pred_test_log_k = model.predict(x_test_k)\n",
    "y_pred_test_k = np.exp(y_pred_test_log_k).ravel()\n",
    "y_test_k_orig = np.exp(y_test_log_k).ravel()\n",
    "\n",
    "training_time_k = end_time_k - start_time_k\n",
    "\n",
    "def mean_relative_squared_error(y_true, y_pred):\n",
    "    return np.mean(((y_true - y_pred) / y_true)**2)\n",
    "\n",
    "r2_k = r2_score(y_test_k_orig, y_pred_test_k)\n",
    "mse_k = mean_squared_error(y_test_k_orig, y_pred_test_k)\n",
    "mae_k = mean_absolute_error(y_test_k_orig, y_pred_test_k)\n",
    "mslr_k = mean_squared_log_error(y_test_k_orig, y_pred_test_k)\n",
    "mrse_k = mean_relative_squared_error(y_test_k_orig, y_pred_test_k)\n",
    "\n",
    "eval_metrics_k = {\n",
    "    'Eval_metrics': ['R2 Score', 'MSE', 'MAE', 'MSLE', 'MRSE', 'Training time'],\n",
    "    'Tuned Neural Network': [r2_k, mse_k, mae_k, mslr_k, mrse_k, training_time_k]\n",
    "}\n",
    "\n",
    "df_metrics_k = pd.DataFrame(eval_metrics_k)\n",
    "print(\"\\n--- Final Model Performance ---\")\n",
    "print(tabulate(df_metrics_k.round(4), headers='keys', tablefmt='pretty', showindex=False))\n",
    "\n",
    "'''# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mean_absolute_error'], label='Training MAE')\n",
    "plt.plot(history.history['val_mean_absolute_error'], label='Validation MAE')\n",
    "plt.title('Model MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('nn_tuned_training_history.png')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b016d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PyTorch Neural Network Implementation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set device for GPU acceleration if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Prepare data\n",
    "data = pd.read_csv('csv/final_dataset.csv')\n",
    "X = data.iloc[:, 2:]\n",
    "Y = data['temp_measured']\n",
    "\n",
    "x_train, x_test, y_train_log, y_test_log = train_test_split(X, np.log(Y), test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Convert to PyTorch tensors and move to device\n",
    "x_train_tensor = torch.FloatTensor(x_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train_log.values).reshape(-1, 1).to(device)\n",
    "x_test_tensor = torch.FloatTensor(x_test_scaled).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test_log.values).reshape(-1, 1).to(device)\n",
    "\n",
    "# Improved neural network architecture\n",
    "class ImprovedTemperatureMLP(nn.Module):\n",
    "    def __init__(self, input_size, dropout_rate=0.3):\n",
    "        super(ImprovedTemperatureMLP, self).__init__()\n",
    "        \n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(1024, 512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Linear(512, 256),\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Linear(256, 128),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout_rate * 0.5)  # Reduced dropout for final layers\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Linear(128, 64),\n",
    "                nn.BatchNorm1d(64),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout_rate * 0.3)\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        self.output_layer = nn.Linear(64, 1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        \n",
    "        for layer in self.hidden_layers:\n",
    "            residual = x if x.size(1) == layer[0].in_features else None\n",
    "            x = layer(x)\n",
    "            # Add residual connection where dimensions match\n",
    "            if residual is not None and x.size(1) == residual.size(1):\n",
    "                x = x + residual\n",
    "        \n",
    "        return self.output_layer(x)\n",
    "\n",
    "# Enhanced early stopping class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=25, min_delta=1e-6, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            \n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        self.best_weights = model.state_dict().copy()\n",
    "\n",
    "# Create validation split\n",
    "x_train_val, x_val, y_train_val, y_val = train_test_split(\n",
    "    x_train_tensor, y_train_tensor, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model = ImprovedTemperatureMLP(x_train_scaled.shape[1]).to(device)\n",
    "\n",
    "# Loss function and optimizer with improved settings\n",
    "criterion = nn.SmoothL1Loss()  # More robust than Huber for regression\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.01, weight_decay=1e-4, eps=1e-8)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=50, T_mult=2, eta_min=1e-6\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(x_train_val, y_train_val)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=0)\n",
    "\n",
    "# Training with validation\n",
    "start_time = time.time()\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "learning_rates = []\n",
    "early_stopping = EarlyStopping(patience=30, min_delta=1e-6)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(1000):  # Reduced max epochs with better early stopping\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    # Step scheduler\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    learning_rates.append(current_lr)\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 25 == 0:\n",
    "        print(f'Epoch {epoch:4d} | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f} | LR: {current_lr:.2e}')\n",
    "    \n",
    "    # Early stopping check\n",
    "    if early_stopping(avg_val_loss, model):\n",
    "        print(f'Early stopping at epoch {epoch}')\n",
    "        break\n",
    "\n",
    "end_time = time.time()\n",
    "training_time_pytorch = end_time - start_time\n",
    "\n",
    "# Final evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Test predictions\n",
    "    y_pred_log = model(x_test_tensor).cpu().numpy()\n",
    "    y_pred_test = np.exp(y_pred_log).ravel()\n",
    "    y_test_orig = np.exp(y_test_log).ravel()\n",
    "    \n",
    "    # Training predictions for comparison\n",
    "    y_pred_train_log = model(x_train_tensor).cpu().numpy()\n",
    "    y_pred_train = np.exp(y_pred_train_log).ravel()\n",
    "    y_train_orig = np.exp(y_train_log).ravel()\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "def mean_relative_squared_error(y_true, y_pred):\n",
    "    return np.mean(((y_true - y_pred) / y_true) ** 2)\n",
    "\n",
    "# Test metrics\n",
    "r2_pytorch = r2_score(y_test_orig, y_pred_test)\n",
    "mse_pytorch = mean_squared_error(y_test_orig, y_pred_test)\n",
    "mae_pytorch = mean_absolute_error(y_test_orig, y_pred_test)\n",
    "msle_pytorch = mean_squared_log_error(y_test_orig, y_pred_test)\n",
    "mrse_pytorch = mean_relative_squared_error(y_test_orig, y_pred_test)\n",
    "\n",
    "# Training metrics (to check for overfitting)\n",
    "r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "\n",
    "# Store results\n",
    "pytorch_metrics = {\n",
    "    'Eval_metrics': ['R2 Score', 'MSE', 'MAE', 'MSLE', 'MRSE', 'Training time', 'R2 Train'],\n",
    "    'PyTorch Model': [r2_pytorch, mse_pytorch, mae_pytorch, msle_pytorch, mrse_pytorch, training_time_pytorch, r2_train]\n",
    "}\n",
    "\n",
    "pytorch_df_metrics = pd.DataFrame(pytorch_metrics)\n",
    "pytorch_df_metrics.to_csv('pytorch_improved_metrics.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMPROVED PYTORCH NEURAL NETWORK RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(tabulate(pytorch_df_metrics.round(4), headers='keys', tablefmt='pretty', showindex=False))\n",
    "\n",
    "# Plot training curves\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(train_losses, label='Training Loss', color='blue', alpha=0.7)\n",
    "ax1.plot(val_losses, label='Validation Loss', color='red', alpha=0.7)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "ax2.plot(learning_rates, color='green')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Learning Rate')\n",
    "ax2.set_title('Learning Rate Schedule')\n",
    "ax2.set_yscale('log')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Predictions vs Actual (Test set)\n",
    "ax3.scatter(y_test_orig, y_pred_test, alpha=0.6, color='blue')\n",
    "ax3.plot([y_test_orig.min(), y_test_orig.max()], [y_test_orig.min(), y_test_orig.max()], 'r--', lw=2)\n",
    "ax3.set_xlabel('Actual Temperature')\n",
    "ax3.set_ylabel('Predicted Temperature')\n",
    "ax3.set_title(f'Test Set: Actual vs Predicted (R² = {r2_pytorch:.4f})')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals plot\n",
    "residuals = y_test_orig - y_pred_test\n",
    "ax4.scatter(y_pred_test, residuals, alpha=0.6, color='purple')\n",
    "ax4.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "ax4.set_xlabel('Predicted Temperature')\n",
    "ax4.set_ylabel('Residuals')\n",
    "ax4.set_title('Residuals Plot')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pytorch_improved_training_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'scaler': scaler,\n",
    "    'model_config': {\n",
    "        'input_size': x_train_scaled.shape[1],\n",
    "        'dropout_rate': 0.3\n",
    "    }\n",
    "}, 'pytorch_improved_model.pth')\n",
    "\n",
    "print(f\"\\nModel saved as 'pytorch_improved_model.pth'\")\n",
    "print(f\"Training completed in {training_time_pytorch:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdd (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
