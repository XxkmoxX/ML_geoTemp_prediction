{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76190361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['well_sample', 'temp_measured', 'pH', 'Na ', 'K', 'Ca', 'Mg', 'Cl',\n",
      "       'SO4'],\n",
      "      dtype='object')\n",
      "Features of dataset: Index(['pH', 'Na ', 'K', 'Ca', 'Mg', 'Cl', 'SO4'], dtype='object')\n",
      "Number of compenents in features: 7\n",
      "0    137\n",
      "1    137\n",
      "2    137\n",
      "3    137\n",
      "4    150\n",
      "5    116\n",
      "6    165\n",
      "7    140\n",
      "8    115\n",
      "9    115\n",
      "Name: temp_measured, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from tabulate import tabulate\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_squared_log_error, mean_absolute_error, make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('csv/final_dataset.csv')\n",
    "print(data.columns)\n",
    "\n",
    "# Separate features and response variables\n",
    "X = data.iloc[:, 2:]                                # features\n",
    "Y = data['temp_measured']                           # response variable: geothermal reservoir measured temperature\n",
    "print(f'Features of dataset: {X.columns}')\n",
    "print(f'Number of compenents in features: {X.shape[1]}')\n",
    "print(Y.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a95f971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+\n",
      "| Eval_metrics  | MLP Regressor |\n",
      "+---------------+---------------+\n",
      "|   R2 Score    |    0.5649     |\n",
      "|      MSE      |   3282.9729   |\n",
      "|      MAE      |    0.1494     |\n",
      "|     MSLE      |    37.1181    |\n",
      "|     MRSE      |    0.2125     |\n",
      "| Training time |    26.8384    |\n",
      "+---------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "### Scikit-learn MLP Implementation ..... probar RandomizedSearchCV u Optuna\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "start_time_mlp = time.time()\n",
    "\n",
    "x_train_mlp, x_test_mlp, y_train_log_mlp, y_test_log_mlp = train_test_split(X, np.log(Y), test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "x_train_mlp = scaler.fit_transform(x_train_mlp)\n",
    "x_test_mlp = scaler.transform(x_test_mlp)\n",
    "\n",
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes=(1024, 512, 256),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.001,  # Regularización L2\n",
    "    learning_rate='adaptive',\n",
    "    learning_rate_init=0.0001,\n",
    "    max_iter=1000,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.2,\n",
    "    n_iter_no_change=20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "mlp.fit(x_train_mlp, y_train_log_mlp)\n",
    "\n",
    "end_time_mlp = time.time()\n",
    "\n",
    "y_pred_log_mlp = mlp.predict(x_test_mlp)\n",
    "y_pred_test_mlp = np.exp(y_pred_log_mlp)\n",
    "y_test_mlp_orig = np.exp(y_test_log_mlp)\n",
    "\n",
    "def mean_relative_squared_error(Y_true, Y_pred):\n",
    "    return np.mean(((Y_true - Y_pred) / Y_true) ** 2)\n",
    "\n",
    "\n",
    "r2_mlp = r2_score(y_test_mlp_orig, y_pred_test_mlp)\n",
    "mse_mlp = mean_squared_error(y_test_mlp_orig, y_pred_test_mlp)\n",
    "mslr_mlp = mean_squared_log_error(y_test_mlp_orig, y_pred_test_mlp)\n",
    "mae_mlp = mean_absolute_error(y_test_mlp_orig, y_pred_test_mlp)\n",
    "mrse_mlp = mean_relative_squared_error(y_test_mlp_orig, y_pred_test_mlp)\n",
    "\n",
    "training_time_mlp = end_time_mlp - start_time_mlp\n",
    "\n",
    "mlp_metrics = {\n",
    "    'Eval_metrics': ['R2 Score', 'MSE', 'MAE', 'MSLE', 'MRSE', 'Training time'],\n",
    "    'MLP Regressor': [r2_mlp, mse_mlp, mslr_mlp, mae_mlp, mrse_mlp, training_time_mlp]\n",
    "}\n",
    "\n",
    "df_mlp = pd.DataFrame(mlp_metrics)\n",
    "df_mlp.to_csv('metrics/metrics_mlp.csv', index=False)\n",
    "\n",
    "print(tabulate(df_mlp.round(4), headers='keys', tablefmt='pretty', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9853620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/venv/cdd/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "/home/chris/venv/cdd/lib/python3.12/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "+---------------+---------------+\n",
      "| Eval_metrics  | NN TensorFlow |\n",
      "+---------------+---------------+\n",
      "|   R2 Score    |    0.5827     |\n",
      "|      MSE      |   2978.1181   |\n",
      "|      MAE      |    36.557     |\n",
      "|     MSLR      |    0.1303     |\n",
      "|     MRSE      |    0.2163     |\n",
      "| Training time |    44.852     |\n",
      "+---------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "### Neural Network implementation (Keras)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import RobustScaler  # Better for handling outliers\n",
    "\n",
    "\n",
    "start_time_k = time.time()\n",
    "\n",
    "x_train_k, x_test_k, y_train_log_k, y_test_log_k = train_test_split(X, np.log(Y), test_size=0.2, random_state=36)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "x_train_k = scaler.fit_transform(x_train_k)\n",
    "x_test_k = scaler.transform(x_test_k)\n",
    "\n",
    "# Define neural networks architecture with LeakyReLU activation\n",
    "model = Sequential([\n",
    "    # Input layer\n",
    "    Dense(512, input_dim=x_train_k.shape[1], kernel_regularizer=l2(0.01)),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Hidden layers\n",
    "    Dense(256, kernel_regularizer=l2(0.01)),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(128, kernel_regularizer=l2(0.01)),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    #Dense(64, kernel_regularizer=l2(0.01)),\n",
    "    #LeakyReLU(alpha=0.1),\n",
    "    #BatchNormalization(),\n",
    "    #Dropout(0.3),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "              loss='mean_squared_error', \n",
    "              metrics=['mean_absolute_error'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           patience=20, \n",
    "                           restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                              actor=0.2, \n",
    "                              patience=10, \n",
    "                              min_lr=1e-4)\n",
    "\n",
    "training = model.fit(x_train_k, y_train_log_k, epochs=2000, validation_split=0.2, batch_size=20,\n",
    "                     verbose=0, callbacks=[early_stop, reduce_lr])\n",
    "\n",
    "end_time_k = time.time()\n",
    "\n",
    "#model.save('keras_nn_model.h5')\n",
    "#print(\"Model saved to 'keras_nn_model.h5'.\")\n",
    "\n",
    "y_pred_test_log_k = model.predict(x_test_k)\n",
    "y_pred_train_log_k = model.predict(x_train_k)\n",
    "\n",
    "y_pred_test_k = np.exp(y_pred_test_log_k) \n",
    "y_pred_train_k = np.exp(y_pred_train_log_k)\n",
    "y_train_k = np.exp(y_train_log_k)\n",
    "y_test_k = np.exp(y_test_log_k)\n",
    "\n",
    "Y_test_k = np.squeeze(y_test_k)\n",
    "Y_pred_test_k = np.squeeze(y_pred_test_k)\n",
    "\n",
    "training_time_k = end_time_k - start_time_k\n",
    "\n",
    "def mean_relative_squared_error(y_true, y_pred_test):\n",
    "    return np.mean(((y_true - y_pred_test)/y_true)**2)\n",
    "\n",
    "r2_k = r2_score(y_test_k, y_pred_test_k)\n",
    "mse_k = mean_squared_error(y_test_k, y_pred_test_k)\n",
    "mae_k = mean_absolute_error(y_test_k, y_pred_test_k)\n",
    "mslr_k = mean_squared_log_error(y_test_k, y_pred_test_k)\n",
    "mrse_k = mean_relative_squared_error(Y_test_k, Y_pred_test_k)\n",
    "\n",
    "\n",
    "eval_metrics_k = {\n",
    "    'Eval_metrics': ['R2 Score', 'MSE', 'MAE', 'MSLR', 'MRSE', 'Training time'],\n",
    "    'NN TensorFlow': [r2_k, mse_k, mae_k, mslr_k, mrse_k, training_time_k]\n",
    "}\n",
    "\n",
    "df_metrics_k = pd.DataFrame(eval_metrics_k)\n",
    "df_metrics_k.to_csv('metrics/metrics_nn.csv', index=False)\n",
    "\n",
    "print(tabulate(df_metrics_k.round(4), headers='keys', tablefmt='pretty', showindex=False))\n",
    "\n",
    "### reducir learning_rate de Adam de 0,01 a 0,001 mejor el r2, \n",
    "### de 0,001 a 0,0001 no lo mejoro y aumento mucho el tiempo.\n",
    "### Agregar input layer de 1024 no mejoro nada y aumento el tiempo\n",
    "### Con RobustScaler mejoro respecto de StandardScaler.\n",
    "### Prueba l2 de 0.01 a 0.001, no mejora nada, queda 0.01"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdd (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
